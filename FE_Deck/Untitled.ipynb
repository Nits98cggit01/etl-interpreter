{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd8c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6bea0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "     -------------------------------------- 310.8/310.8 MB 3.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     -------------------------------------- 200.5/200.5 kB 3.1 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285411 sha256=272e36d4d3e3384251811faaf3a31a798e18cc96f0584df789ad73f66c35ebc4\n",
      "  Stored in directory: c:\\users\\nitins\\appdata\\local\\pip\\cache\\wheels\\2b\\9a\\39\\d8019ffbfb76a39433455e3d5799e94d3e3cae8f41229f6bf8\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardscdm = pd.read_csv(r\"C:\\Users\\NITINS\\OneDrive - Capgemini\\CAPGEMINI\\PROJECT\\GEN AI\\FE_Deck\\cardscdm_2201.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28a28976",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "# Define the source CSV file path\n",
    "csv_file_path = \"C:\\\\Users\\\\NITINS\\\\OneDrive - Capgemini\\\\CAPGEMINI\\\\PROJECT\\\\GEN AI\\\\FE_Deck\\\\cardscdm_2201.csv\"\n",
    "\n",
    "# Load data from the CSV file\n",
    "source_data = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3496224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform transformations\n",
    "# 1. Sorting data based on a column\n",
    "sorted_data = source_data.orderBy(\"AMRL_ACCEPT_DECLINE_IND\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c78647b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Filtering data\n",
    "filtered_data = sorted_data.filter(col(\"AMRL_NBR_OF_ASSOCS\") >= 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "899d3f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[ORG: int, TYPE: int, AMRL_NBR_OF_ASSOCS: int, AMRL_STATUS: int, AMRL_LAST_MAINT_OPER: string, AMRL_ACCEPT_DECLINE_IND: string, AMRL_DECISION_DATE: string, AMRL_EDIT_OPER: string]\n"
     ]
    }
   ],
   "source": [
    "# 3. Executing SQL query\n",
    "filtered_data.createOrReplaceTempView(\"temp_table\")\n",
    "sql_query = \"\"\"\n",
    "    SELECT\n",
    "        ORG,TYPE,AMRL_NBR_OF_ASSOCS,AMRL_STATUS,AMRL_LAST_MAINT_OPER,AMRL_ACCEPT_DECLINE_IND,AMRL_DECISION_DATE,AMRL_EDIT_OPER\n",
    "    FROM\n",
    "        temp_table\n",
    "    WHERE\n",
    "        AMRL_ACCEPT_DECLINE_IND = 'A'\n",
    "\"\"\"\n",
    "result_data = spark.sql(sql_query)\n",
    "\n",
    "# Define the target table\n",
    "target_table = \"target_table_name\"\n",
    "\n",
    "print(result_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9116a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ORG  TYPE  AMRL_NBR_OF_ASSOCS  AMRL_STATUS AMRL_LAST_MAINT_OPER  \\\n",
      "0   702    40                   3            1                  6LC   \n",
      "1   702    40                   2            1                  6LC   \n",
      "2   702    40                   3            1                  MFS   \n",
      "3   702    40                   3            1                  6LC   \n",
      "4   702    40                   3            1                  6LC   \n",
      "5   702    40                   2            1                  MFS   \n",
      "6   702    40                   3            1                  6LC   \n",
      "7   702    40                   3            1                  MFS   \n",
      "8   702    40                   3            1                  MFS   \n",
      "9   702    40                   3            1                  6LC   \n",
      "10  702    40                   2            1                  6LC   \n",
      "11  702    40                   3            1                  VG7   \n",
      "12  702    40                   3            1                  6LC   \n",
      "13  702    40                   3            1                  6LC   \n",
      "14  702    40                   3            1                  6LC   \n",
      "\n",
      "   AMRL_ACCEPT_DECLINE_IND AMRL_DECISION_DATE AMRL_EDIT_OPER  \n",
      "0                        A          18-Jan-21            6LC  \n",
      "1                        A          12-Jan-21            6LC  \n",
      "2                        A          04-Jan-21            MFS  \n",
      "3                        A          19-Jan-21            6LC  \n",
      "4                        A          21-Jan-21            6LC  \n",
      "5                        A          05-Jan-21            MFS  \n",
      "6                        A          12-Jan-21            6LC  \n",
      "7                        A          05-Jan-21            MFS  \n",
      "8                        A          07-Jan-21            MFS  \n",
      "9                        A          05-Jan-21            6LC  \n",
      "10                       A          05-Jan-21            6LC  \n",
      "11                       A          05-Jan-21            6LC  \n",
      "12                       A          04-Jan-21            6LC  \n",
      "13                       A          05-Jan-21            6LC  \n",
      "14                       A          04-Jan-21            6LC  \n"
     ]
    }
   ],
   "source": [
    "target_file_path = \"C:\\\\Users\\\\NITINS\\\\OneDrive - Capgemini\\\\CAPGEMINI\\\\PROJECT\\\\GEN AI\\\\FE_Deck\\\\transformed_data.xlsx\"\n",
    "\n",
    "# Convert result_data to a pandas DataFrame\n",
    "result_df = result_data.toPandas()\n",
    "print(result_df)\n",
    "# Write the pandas DataFrame to a CSV file\n",
    "result_df.to_excel(target_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419d1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "# Define the source CSV file path\n",
    "csv_file_path = \"C:\\\\Users\\\\NITINS\\\\OneDrive - Capgemini\\\\CAPGEMINI\\\\PROJECT\\\\GEN AI\\\\FE_Deck\\\\cardscdm_2201.csv\"\n",
    "\n",
    "# Load data from the CSV file\n",
    "source_data = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Perform transformations\n",
    "# 1. Sorting data based on a column\n",
    "sorted_data = source_data.orderBy(\"AMRL_ACCEPT_DECLINE_IND\")\n",
    "\n",
    "# 2. Filtering data\n",
    "filtered_data = sorted_data.filter(col(\"AMRL_NBR_OF_ASSOCS\") >= 2)\n",
    "\n",
    "# 3. Executing SQL query\n",
    "filtered_data.createOrReplaceTempView(\"temp_table\")\n",
    "sql_query = \"\"\"\n",
    "    SELECT\n",
    "        ORG,TYPE,AMRL_NBR_OF_ASSOCS,AMRL_STATUS,AMRL_LAST_MAINT_OPER,AMRL_ACCEPT_DECLINE_IND,AMRL_DECISION_DATE,AMRL_EDIT_OPER\n",
    "    FROM\n",
    "        temp_table\n",
    "    WHERE\n",
    "        AMRL_ACCEPT_DECLINE_IND == 'A'\n",
    "\"\"\"\n",
    "result_data = spark.sql(sql_query)\n",
    "result_df = result_data.toPandas()\n",
    "print(result_df)\n",
    "result_df.to_excel(target_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c810a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7536d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
